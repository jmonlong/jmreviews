---
sidebar: true
title: Structural Variation Methods
bibliography: [../../static/library-small.bib]
link-citations: true
weight: 5
output:
  blogdown::html_page:
    toc: true
---

Different technologies and methods have been used over the years to detect structural variation.
The first genome-wide assessment tool was the hybridization array.
Recently, high-throughput sequencing has shown better resolution.
The next step should be long-read sequencing technologies which will allow interrogation of the remaining challenging regions and SV types.

# Cytogenetic techniques

Aneuploidy and extremely large SVs can be seen in directly in the karyotype, especially after G-banding (~1960-70s).
In theory deletions, translocations and inversions can be seen in the karyotype.

Fluorescent in situ hybridization (FISH) uses fluorescent probes that bind to complementary DNA in the genome.
Developed in the 1980s, it's used to detect the presence or absence of specific DNA sequences in the sample.
Many probes come from DNA fragments that were isolated previously and kept in the form of bacterial artificial chromosomes.
Briefly, the cells and chromosomes are fixed and the probe is incubated for several hours, after which a microscope inspection reveals the location of the bindings.


# Array-CGH technology

One could argue that aCGH is a cytogenetic technique? 
Anyway, it was such a big improvement in the CNV detection field that it deserves its own section.

DNA from a test sample and normal reference sample are labeled differentially, using different fluorophores, and hybridized to several thousand probes.
The probes are derived from most of the known genes and non-coding regions of the genome, printed on a glass slide.

![Protocol](/Array-CGH_protocol.svg)

Using this method, copy number changes at a level of **5â€“10 kilobases** of DNA sequences can be detected.
As of 2006, even high-resolution CGH (*HR-CGH*) arrays are accurate to detect structural variations (SV) at resolution of **200 bp**.

This method allows one to identify new recurrent chromosome changes such as microdeletions and duplications in human conditions such as cancer and birth defects due to chromosome aberrations.
This technology is not able to detect balanced chromosomal imbalances as translocations or inversions.

## Circular binary segmentation

For example improved by @Venkatraman2007, CBS iteratively identify segment with different mean value.

It computes a maximal t-statistic \\(T=max_{1 \le i < j \le m}\|T_{ij}\|\\), where \\(T_{ij}\\) compared the mean of observations between *i+1* and *j* to the rest using the formula:

$$ T_{ij} = \frac{\bar{Y}_{ij}-\bar{Z}_{ij}}{s_{ij}\{(j-i)^{-1}+(m-j+i)^{-1}\}^{1/2}} $$

where $$\bar{Y}_{ij}$$ and $$\bar{Z}_{ij}$$ are the mean values within and outside of *i* and *j*.

## Shifting Level Model

@Magi2010 model the \\(log_2\\) ratios as the sum of two independent stochastic processes.
One is the biological process and results from variation in DNA copy number while the second one models the noise due to technical errors.

# SNP-array

Some methods are implemented for normal genomes (PennCNV [@Chen2013], iPattern, QuantiSNP, Birdsuite, Genotyping Console).
Others are capable of dealing with complex signal in tumoral samples (ASCAT [@VanLoo2010], GAP [@Popova2009]).

Concordance between the results of different methods is often weak, pushing several studies to adopt an ensemble approach. In this approach, several (2-3 usually) methods are ran on the same SNP-array and discordant calls are filtered [@Walker2013].



# Next-generation sequencing

Compared to Array-CGH technologies, High-Throughput Sequencing allows for better breakpoints resolution, identification of a broader range of structural variant types and better estimation of larger Copy-Number Variation.
It also has the potential to identify allele-specific structural variation.


## Read-Depth (RD)

In this approach, the **genome is partitioned** into windows, usually non-overlapping, and the **number of reads in each window** is used as a **read depth(RD)** measure.
Most of the time, random distribution of the sequenced reads across the genome is assumed.
Yet several technical factors affect read depth (GC content, open regions, mappability) and are not completely corrected for.

### RD in one sample

In @Campbell2008 **circular binary segmentation** algorithm is adapted.
Two lung cancer cell lines were sequenced using GenomeAnalyzer:  paired end 29-36bp reads with ~200/400 bp insert size.
The main discovery approach used paired end information; RD was used as additional information.

Coverage was normalized using custom bin size estimated from a simulation analysis to standardize the number of unique mapping.
It resulted in ~425 simulated reads per bin and an average bin size of 15 Kbp.
Region within 1Mb of the centromere or telomere were discarded.
To detect copy number variation, a method was adapted from array CGH approach using a fast version of circular binary segmentation algorithm, implemented in *DNACopy} package from Bioconductor [@Venkatraman2007].

The minimum event detected was 30 Kbp long.
They also noticed some amplicon amplification with up to 35-50 fold amplification.

---

**RDXplorer** aka Event-Wise Testing is used on single read sequencing results(~30X coverage) from Genome Analyzer [@Yoon2009].
The method was applied to five samples (CEU trio: NA12878 NA12891 NA12892 + NA18507 YH).

The genome was cut into 100 bp windows in order to ensure breakpoint identification accuracy and small event detection.
The normality was assessed with QQplot and loosely accepted.
GC bias was corrected by determining the deviation in coverage of each GC class and linear correction.
The RD is converted into a Z-score (dividing by the genome average and standard deviation).
Duplication and deletion were separated by the computation of two different P-values \\(p^U_i=P(Z>z_i)\\) and \\(p^L_i=P(Z<z_i)\\) in order to join consistent signal.
Based on this significance testing, small events meeting some significance are first identified and clustered together and then grouped into larger events.
Then to assess the significance of *l* consecutive windows *A* among *L* total windows(here in the chromosome), \\(max\{p^U_i|i\in A\}\\) and \\(max\{p^L_i|i\in A\}\\) were compared to \\((\frac{FPR}{L/l})^{\frac{1}{l}}\\) to control for a false-positive rate FPR.
In practice, to join the small events, the size of window interval was incremented and tested until losing significance(here FDR 5%).
Some additional filters are also necessary to clean up the results: minimum copy number deviation, small clusters merging, significance threshold choice based on manual inspection of results.
The copy number was estimated by rounding up \\(\frac{2 \times read~count}{mean~read~count}\\).

The results were compared across the five samples to define poly/monomorphic events.
To validate their call they used the data from the Genome Structural Variation (NimbleGen tiling array) as reference.
However, instead of overlapping the SVs for the correspondent samples, they used the SVs from the whole set of 40 samples.
Finally, they compared their results to the one from PEM approaches on the same samples.
The SVs detected being very different between the two techniques, the conclusion was that both methods had his advantages to different types of SVs.

---

**mrFAST** was used to map more robustly reads to duplicated regions of the human genome [@Alkan2009].
Previous knowledge on the copy number state guided the parameter of the segmentation approach.
In the end, 6 out of 7 consecutive bins (5 Kbp) with a deviation exceeding 3 SD defined a CNV call.
They also corrected for GC bias using a LOESS-based smoothing technique.

---

**Control-FREEC** can run on one sample although it is recommended to provide a control.
More on the method [below](#controlfreec).

---

**CNVnator** combines mean-shift approach with additional refinements as GC correction and multiple-bandwidth partitioning [@Abyzov2011].
It rapidly (a few hours) analyze small bins (around 100bp) to detect CNVs from a wide range of sizes (hundreds to megabases in length).

GC bias was corrected by binning and average adjustment.
The bins were partitioned into segment using a mean-shift technique, that comes from image processing.
Briefly, bins point at the closest neighboring bin in a certain bandwidth and breakpoint are identified by two bins diverging from each other.
They improved the bandwidth estimation strategy.
Then they compare the average bin counts in the segments as well as the neighboring segments.
To deal with the issue of multiple mapping and the potentially duplicated calls, they record the fraction of reads with mapping quality zero.
Additionally they normalized the counts by the global coverage to estimates count, ultimately rounding off these values.

They used two 1KGP trios to test the reproducibility of their calls and stressed the importance of coverage uniformity compared to actual sequencing depth.
A minimum of 50% overlap defined concordance between two calls.
Then they simply checked the general direction when comparing parent-parent and parent-child overlap.
After explaining the challenges of duplication detection, they noticed 50% of their duplications located close to gaps.
Finally, they explored potential de novo or multi-allelic variants.
If variants are present in parent they could create de novo or multi-allelic event.
They didn't validate any but mentioned that the immunoglobulin lambda locus was potentially true somatic variants.

---

**cnD** (Jared Simpson and Richard Durbin) is HMM approach using RD and allele imbalance to call CNVs from homozygous strains of mice.

---

**CNV-TV** uses total variation penalized least squares model to detect changes in RD of a sample [@Duan2013a].
It improves the segmentation.
Apparently better across a wider range of sizes and copy numbers compared to other methods. 
Also better breakpoint accuracy. 

The figures are a bit unusual, maybe the authors' background leans more toward math/CS?

---

**CNVfinder** uses HMM to model bias and coverage variation using a negative binomial [@McCallum2013].
After normalization the coverage follows even less a Poisson so a negative binomial is better suited.
The coverage, GC content and mappability are included in the emission probabilities.

The package name changed from the beginning of the paper to the end (CNVwire).

---

**@Sepulveda2013** developed a more flexible Poisson model to analyze genomes of micro-organisms, such as malaria.
The rate of the Poisson distribution varies across the genome according to a Gamma (negative binomial) or Lognormal distribution.

---

**GENSENG** [@Szatkiewicz2013] uses a complex HMM strategy to fit different models in order to take into account biases.
A negative binomial is used to account to over-dispersion and the effect of confounders.
Unknown confounders are taken into account by the over-dispersion parameter and a noise component in a mixture model.
The input is a triplet of RD, GC content and mappability for each bin.
They used 500 bp windows with 200 bp overlap (why not).

---

**GROM-RD** [@Smith2015] aims at removing genomic biases before performing segmentation.
It's unique features include excessive coverage masking, GC bias mean and variance normalization, GC weighting, dinucleotide repeat bias correction and usage of bins of mutliple sizes.
Masking regions with excessive coverage is actually used only in the *no-repeat* mode and the method is also ran on the unmasked genome with both call sets outputted in the end.
The GC content is computed from the GC weights which are computed at the base level using bases within the average insert size.
Quantile normalization the ensures that both the mean and variance in RD are normalized per GC class.

A big "recipe" with different analysis for different regions. 
Comprehensive but also a lot of arbitrary thresholds.

### Case vs Control

The RD in the case sample is compared to the control sample. The concept is similar to aCGH, but with much more data points and control.

---

**SegSeq** [@Chiang2009] uses a different approach, it looks for breakpoints comparing tumor-normal log-ratio of two consecutive windows.
Random distribution of the sequenced read and Poisson is assumed.
Using the normal sample as control should take care of most of the GC bias and mappability.
It's not clear in the main text how the RD from the two samples are normalized.

In more details, each tumor read position is first tested for being at a breakpoint location by extending two windows to the left and right by *w* normal read.
A P-value is derived from the differences between the two log-ratios.
Candidate breakpoints (\\(P-value>p_{init}\\)) are then eliminated by recomputing the P-value using the whole segments and tested against a minimal \\(p_{merge}\\).

The events detected in their application ranged from 29kb to 582kb.

---

**CNV-seq** [@Xie2009] tries to estimate the best window size and uses \\(log_2\\) copy ratio between two samples and a sliding-window approach.
It uses a Poisson model and assumes the reads are randomly distributed across the genome.
Heavily inspired by aCGH technology.
Statistical analysis of power according to coverage but practical use not really described/clear.

---

**CNAseg** [@Ivakhno2010] segments RD signal using a HMM tumor-normal approach.
Notably, the difference in read counts between normal and tumor is used and modeled by Skellam distribution.

Normal and tumor read counts are normalized by their total number of reads beforehand.
GC bias is corrected using LOESS regression and mappability issues are *taken care* of by smoothing the signal with Discrete Wavelet Transform.

On the last steps, segments are merged using a \\(\chi^2\\) test.
The threshold for the test is derived by mixing flowcells from normal and tumor and running control segmentation expected to contain only noise.

---

**ExomeCNV** [@Sathirapongsasuti2011] looks at the coverage and B allele frequency in exome sequencing data.

---

**BIC-seq** [@Xi2011] minimize the Bayesian information criterion in order to get the best segmentation of the tumor/normal RD ratio.

---

**VarScan 2** [@Koboldt2012] apply log ratio approach to exome data, controlling carefully for similar sequencing of the normal and tumor sample(similar hybridization).

**+++**

---

###### {#controlfreec}

**Control-FREEC** [@Boeva2011;@Boeva2012] aims at predicting copy number variation along with LOH.
The read depth can be corrected for GC bias and mappability if a control sample is unavailable.
For cancer, the method can also estimate the stromal contamination or ploidy.
Both germline and somatic variants can be called.

The read depth is computed in windows tiling the genome.
The RD in the sample is normalized by dividing by the *expected read-depth*.
If a control sample is provided, the *expected read-depth* is computed by fitting the RD of the sample vs control, forcing an intercept of 0 (cubic polynomial recommended).
If not, the GC content is fitted using a cubic polynomial to provide an *expected read-depth*.
A mappability track, either provided or as the proportion of Ns in the window, is used to scale the *expected read-depth*.
Of note, centromeric/telomeric regions are excluded using a configurable distance threshold (default 50 kbp).
The normalized RD is segmented using a lasso-based algorithm that is robust to outliers.
The main parameter for the segmentation is the number of breakpoints and the tool will explore an increasing number of breakpoints, stopping once the residual sum of squares stops decreasing.

In its latest version, it integrates allelic imbalance estimates.
The B-allele frequency (BAF) of known SNPs is used for both segmentation and genotyping.
First, the windows are segmented using the same lasso-based algorithm on the median BAF deviation from expectation (0.5).
This new set of breakpoints is merged with the RD-based breakpoints.
The copy number in each segment is based on the RD but the genotyping on the BAF (maximum likelihood using Gaussian mixture models).

---

**CNAnorm** [@Gusnanto2012] corrects for cancer genome size and tumour cell content to segment tumor/normal RD ratios.


---

**CoNVEX** [@Amarasinghe2013] is an exome approach using discrete wavelet transform on tumor/normal coverage ratio in each exonic region. The segmentation is performed through Hidden Markov Models.

---

**WaveCNV** [@Holt2013] uses a translation-invariant discrete wavelet transform to detect CNV breakpoints from tumor/normal RD ratios.

---

**AbsCN-seq** [@Bao2014] is an exome approach to detect tumoral variant taking into account purity and ploidy.

---

**CONSERTING** [@Chen2015] uses regression trees to segment the tumor/normal coverage ratio.

Interestingly the breakpoint are fine-tuned using local SV detection around the segmented calls with *CREST*.

### Choosing the optimal window size

Control-FREEC defines the optimal window size using the theoretical coefficient of variation in a window [@Boeva2011].
This CV is computed from the total read count, assuming a Poisson distribution. 
The authors suggest a CV from 0.05 and 0.1.


## RD in several samples

**JointSLM** [@Magi2011] simultaneously analyzes several samples to increase statistical power to detect shared events.
It can detect common CNVs as small as 500 bp in their analysis.

Uniform distribution of the reads is assumed to use and model the count data as the same Poisson for the entire genome.
A specific Hidden-Markov Model, coined Shifting Level Model (SLM) model the log2 ratios as a mixture of a white noise process and a biological process.
100bp bins were corrected for GC, median-normalized and approximated by a Normal distribution.
The user-defined parameters are: the transition probability from a state(here CN across all samples), the proportion of the signal variance due to the biological process, and the number of states.
The other state parameters are first estimated through Baum and Welch algorithm.
Finally the best state sequence is estimated by Viterbi.

To test performance, synthetic chromosomes were constructed by mixing bins from chromosome 1 and X from a male individual.
Tweaking the input parameters they obtain good results...
Then they compare the results from three other methods (ran using their default parameters) and as expected their parameter-tweaked method performs better! 
In practice they recommend to call event of size 500 bp or more, but, at this resolution, JointSLM is not that much better.
Although I find this synthetic chromosome approach clever, it's not that realistic (e.g. doesn't recapitulate repeat context) and only works for deletions.

For their real data analysis, they mixed 2 trios and 2 unrelated individuals and used a "conservative" (but arbitrary) set of parameters to get a round number of detected CNVs (3000).
They notice an over-representation of calls in centromere/telomeres (many systematic errors I would guess).
Using known annotation (DGV, then GSV) JointSLM had 58% specificity and 43% sensitivity, much lower than the simulation results.

These results were compared to PEM (not a RD approach) and found that JointSLM, designed to find CNV from RD, was better, according to the truth (aCGH CNV calls), compared to PEM approaches designed for deletion/insertions.

Finally, they used the copy number to cluster regions and samples.
They found two clear clusters that represented sample ancestry.

---

**Copynumber** [@Nilsen2012] is a R package that can analyze single of multiple samples together to segment the genome.

---

**cn.MOPS** [@Klambauer2012]  integrates in their Bayesian model , read depth signal for all samples.
They use a mixture of Poisson distributions to model the copy number of each sample and technical variation.

**+++++**

---

**XHMM** [@Fromer2012] applies a PCA-based normalization of exome read depth and performs coverage segmentation using Hidden Markov Model. The RD is normalized using the mean RD across all samples; regions are filtered out if too variable across the samples.

---

**CoNIFER** [@Krumm2012] is an exome approach that normalize coverage and compute Z-score using information from several samples.

---

**CONTRA** [@Li2012] is another exome approach that segment ratios whose baseline is computed using reference samples.

---

**EXCAVATOR** [@Magi2013] is another exome approach that segment ratios whose baseline is computed using reference samples. It uses Hidden Markov Models.

---

**FishingCNV** [@Shi2013] analyze exome data using information from a set of reference samples.

---

**PatternCNV** [@Wang2014] analyze exome data using information from a set of reference samples.

---

**ONCOCNV** [@Boeva2014] is an amplicon sequencing approach that includes a multi-factor normalization and annotation of large CNVs. The amplicon technology is particularly useful to detect somatic events.

---

**CNVkit** [@Talevich2014] uses off-target reads from targeted sequencing and a series of corrections and normalization.
In a nutshell, non-targeted regions are partitioned in larger bins so that the coverage is comparable to the one in the targeted regions.
Then a series of normalization and corrections are applied and a typical segmentation approach is performed.

First the usefulness of off-targeted reads is verified by showing that copy-number correlated with adjacent targeted regions and aCGH data.
To remove technical noise, the coverage in each bin is normalized by the expected coverage in a set of reference samples.
However they note some biases to vary from sample to sample hence the needs for additional corrections.

For this end, *CNVkit* uses a rolling median technique to re-center bin counts with bins of similar GC content, repetitiveness, larger size and distance from other targets.
While the normalization using reference samples and GC bias correction seem to improve the signal, the fancy correction at the targets boundaries doesn't show so much interest.
Moreover, several samples (out of not so many) are still outliers and will be influenced by technical noise.

Of note, bins are previously filtered if the coverage in the reference samples is too low or too variable, or if the repeat content is too high.
They don't explain how the reference samples are normalized get the average coverage in each bin, they just mention Tuckey's biweight something.

The analysis is mostly done using a Python library and is quite fast (supposedly seconds once the reads are counted).

---

**@Glusman2015** integrates RD information from thousands of samples. Its normalization and reference building may seem simplistic but it is extremely efficient. It is exactly the goal of the pipeline as it targets clinical usage.

---

**Genome STRiP 2.0** [@Handsaker2015] is the extended version of *Genome STRiP*.
Now the read depth across many samples is used to detect multi-allelic CNVs.

Quickly the methods consists in *"carefully normalizing"* the coverage information across samples and fitting a Gaussian mixture model.
The normalization seems to be only GC bias correction.
I'm guessing kind of a sequencing depth normalization but it's not explicit, even in the supplementary material.
The Gaussian mixture model is not described much either, although it should *"integrate sample-specific variation"*.
In practice the genome is scanned and when a regions deviates from an unimodal distribution, the window is fine-tuned to get the best Gaussian mixture model.
Regions where the model did not result in a series of discrete copy numbers were removed.
Complex regions, e.g. mixing both deletions and duplications, or inversions, are not addressed here.


## Methods for repeat-rich regions

Tandem copy-number variation reconstruction by improved multi-mapped reads counting approach was proposed by @He2011.

---

**VariationHunter-CR** [@Hormozdiari2010] improves their previous PEM approach by modeling transposon insertion. It considers and chooses from the potential multi-mapping of a read to support a SV [@Halper-Stromberg2014].

---

**CNVeM** [@Wang2013] uses an Expectation-Maximization to allocate ambiguous reads to the correct location and compute better copy number estimates in repeat-rich regions.

---

**MELT** finds mobile element insertions. It was developed for the 1000 Genomes Project [@Sudmant2015a].
It was also used to confirm and characterize a driver somatic L1 insertion in the APC gene in colorectal cancer [@Scott2016].

Insertions are detected using the classical PEM/split-read approach: reads are selected if mapping to the TE consensus, then filtered and monitored for breakpoint patterns (target site duplication).

It generally uses the transducted sequence to identify the source L1 but apparently can also use internal variants (see @Scott2016).

---

**Mobster** detects mobile element insertion using a similar approach to MELT.

---

[**TraFiC**](https://github.com/cancerit/TraFiC) was used in @Tubio2014 to detect **somatic L1 insertion in cancer**. It can detect **solo-L1** insertions as well as **partnered and orphan transduction**. With this method they could even reconstruct multi-jumps insertions.


## PEM and Split-reads

**@Tuzun2005** seem to be the first who used paired-end mapping to detect SVs.

---

**@Korbel2007** were one of the first to use paired-end mapping information to detect SVs.
They used outlier from the insert size distribution as well as inverted reads or distal insertions.
Looking at the paired-end information they could detect insertion, insertion of distal locus, deletions and inversions.
Two supporting read were necessary to call an event.

*NA15510* and *NA18505* were used in the paper.
The samples were sequenced using 454 technology and ~250 bp reads with ~3 kb insert-size.
Using this technique, events of 3kb or larger could be detected.
They found signal for a slight selective constraint on against SVs in genes, enrichment of retrovirus- and transposition-related proteins.
Several SVs were also found in repetitive elements (*Alu*, L1, L2, SVA, LTRs) with a significant enrichment for L1s.
They estimated the origin of their SVs as mainly non-homologous end-joining (NHEJ) (56%), retrotransposition (30%) and a few non-allelic homologous recombination (NAHR) (14%).

To validate their calls they compared to DGV annotation, Celera assembly, array-CGH and PCR experiments.

---

**@Campbell2008** Two lung cancer cell lines were sequenced using GenomeAnalyzer:  paired end 29-36 bp reads with ~200/400 bp insert size.

Aberrant paired reads supported by high mapping quality were targeted by a confirmatory screening (PCR amplification and conventional sequencing) that gave them the base-pair breakpoint resolution.

Mostly deletions were detected.
Around 50-60% of the SVs found were germlines (*Q?? I still don't know how they distinguish germline/somatic from one cancer sample.*) and several L1 and AluY deletions were found.
Also a bunch of events created fusion transcript or exon duplication.
They observed *"genomic shards"*, i.e. small DNA fragments inserted around breakpoints, and inverted duplications.

---

**BreakDancer** [@Chen2009] also monitors anomalous(in term of insert size or orientation) paired reads to detect large structural variants.
The confidence score for the calls integrates the number of pairs supporting it, the sequencing coverage and the insert size deviation.
The significance of the score is compute for each insert type by assuming an uniform distribution across the genome (not very realistic for repeat-rich regions).
It also propose a module to detect small indels, using the the insert size information of reads mapping to a window(of size the mean insert size), sliding it 1bp at a time.
 It integrates samples pooling to increase detection power of common variants or flag inherited variation in normal vs tumor studies.

---

**SVMiner** [@Pyon2011] looks like a standard approach.

---

**CLEVER** [@Marschall2012] organizes all read into *read alignment graphs* and looks for cliques of consistent reads.

**++**

---

**RetroSeq and SECluster** ++

---

**CommonLAW** [@Hormozdiari2011a] looks at PEM signal across several samples to increase the sensitivity and specificity.

---

**Hydra-Multi** [@Lindberg2014] also looks at PEM signal across several samples to increase the sensitivity and specificity.

---

**Pindel** [@Ye2009] uses pattern growth on the unmapped pair of uniquely mapped anchor read.
The mapped read (anchor) is used to reduce the search-space.
The unmapped read is cut in two or three and the ends are mapped around the anchor.
Breakpoints with at least 2 supporting reads are reported.
Depending on the read length, this approach can detected large deletions (1bp-10kb) and small insertions (1-20bp).

---

**Splitread** [@Karakoc2012] in exome sequencing data, splits the unmapped read into balanced or unbalanced subsequences and detect cluster of sub-reads.
Something about *weighted set-cover approximation*.

**++**

---

**YAHA** [@Faust2012] implement a split read approach for long reads.

**++**

---

**SplazerS** [@Emde2012] is another approach.

---

**Gustaf** [@Trappe2014] is yet another approach.

## Assembly approach

+ How to deal with repeats ? diploidy ?
+ What is the difference between global/local assembly ? (except computational complexity)
+ Why is it so much difficult to assemble (tandem) duplication ?

**Advantages** Firstly, assembled sequence gives extensive information about the breakpoint location and mechanism of formation.
Assembly approaches are also not biased toward the reference genome, which we realize can be significantly different from a studied human genome.
Hence It could theoretically simplify characterization of complex events.
Indeed if a complex event is defined according to its deviation from the reference, it is not particularly complex here.
Of course, other issues will create event that are complex for the assembly exercise.

**Disadvantages** It requires more coverage and is computationally more demanding.

**Overlap-layout-consensus vs de Bruijn graphs** Overlap-based graphs were first implemented.

In overlap-layout-consensus graphs, fragment assembly problem is cast as finding a path visiting every vertex exactly (or at most) once, a Hamiltonian Path Problem.
Here a vertex is a read; edges join reads that overlap.

In de Bruijn graphs, fragment assembly problem is cast as finding a path visiting every edge once, a Eulerian Path Problem.
De Bruijn graphs typically require less resources.
In theory, the graph resolution is also easier when repeats are present.

**Challenges** Diploidy or cell heterogeneity is a major challenge.
Most of the whole-genome assembler aim at producing an haploid version of the sequenced genome, ignoring local structure.

Insertions are also more challenging to assemble.
The full inserted sequence can be problematic to retrieve.
The sequence redundancy is also more complex to assemble (and map/retrieve).

The choice of the k-mer size is very important.
It must be a tradeoff between complexity and coverage.
A small k-mer will create complex structure due to common k-mer in different part of the assembled sequence, but each node will be well covered.

---

**Cortex** [@Iqbal2012] is a *de novo* assembly method that uses de Bruijn graphs to detect and genotype simple and complex variants in an individual or population.

---

**Platypus** [@Rimmer2014] Platypus integrated mapping, assembly and haplotype approaches to call variants.
It also integrates information across multiple samples.

First a set of candidates is derived from read alignment, local assembly or external sources.
The local assembly uses reads mapping in a genomic window, as well as their mate, and reference sequences to construct a colored de Bruijn graph.
Unique paths that begin and end on reference sequence are extracted.
The candidate variants are then clustered and a list of potential haplotypes is defined.
Using reads from all samples, a diploid segregation model and an expectation-maximization approach the haplotype frequencies are derived from likelihood estimates.
These frequencies are finally used as prior to call haplotype in each sample, again by likelihood approximation.

This method seems to be highly sensitive and specific thanks to its comprehensive set of variants detected from the local assembly and realignment process.
It is supposedly not affected by repeat content, although the assembly is limited by the total read coverage and the mate of locally mapped reads.
Still they managed to detect full Alu insertions.
Here SV is described as an extra feature but the focus in on SNP and indel haplotype calling.
Hence few efforts have been directed toward detection of the wide range of SV types and sizes.

Among the remarkable results, the calling rate for de novo variant and HLA analysis are quite impressive.
Of note, Platypus works similarly with Whole-Exome Sequencing data.

---

**TIGRA** [@Chen2014] Targeted Iterative Graph Routing Assembler was widely used in large-scale projects (e.g. 1KGP) for breakpoints assembly of deletions and mobile element insertion.
It's an old effort, that they started in 2008, which was optimized and improved across the years.

To clarify, TIGRA assembles the breakpoint region of SV, not the entire SV region.
Another clarification: TIGRA is used to validate existing calls and reduce the FDR, not so much to characterize SV mechanism or features.

First, reads are extracted around (500bp-1Kbp) the boundaries of the input region.
Mapped reads and their mate are retrieved from the BAM files.
If multiple samples are available and their genotype is known, reads from the variant-containing samples can be pooled.
These reads are then assembled using a de Bruijn graph.
Multiple k-mer sizes are iteratively used, with previously assembled contigs taken as pseudo-reads in the next iterations.
During the contigs construction, tips likely created by sequencing errors are pruned.
Bubbles with less than 3 nucleotides differences are also joined.
Entire reads are used to distinguish repeats, instead of k-mers.
The assembled sequences are then mapped back to the reference genome using cross_match.
Every contig is tested although they don't explain explicitly their how these contigs are retrieved from the graph.
Contigs with more than two hits or substitution rate higher than 0.5% (a bit low, no?) are not reported.
A glocal alignment is used.

They tested TIGRA assembly step against Velvet, SGA, Phrap and SPAdes.
They used datasets with known deletions breakpoints and mobile element insertions.
TIGRA shows the highest proportion of validated SVs in any situations.
They also managed to detect quasi-full length Alu insertion (was it because they knew it was there ?).
TIGRA is more robust to low-coverage alleles than other existing methods, thanks to its multiple k-mer size process.

It seems mainly focused on deletions, duplication (also not benchmarked) and mobile insertion, although they mention being able to detect any type of de novo SV and having specific strategies to deal with inversion or reciprocal translocations.

While it handles repeats slightly better than other methods, by using full reads instead of k-mers, repeated sequences longer than the read length will not be resolved and are bound to create ambiguous graphs.
Indeed they observed a reduction of assembled breakpoints with higher breakpoint homology.
It is implemented to assemble small breakpoint regions, hence require almost nucleotide resolution in its input regions.

They comment about a important drop in validation rate for RD-based regions.
I didn't understand how they define their contigs (used in the iterative process; aligned against the reference).
Is it all possible path in the graph ? Only unambiguous paths ?

---

**SGA** [@Simpson2012] stands for String Graph Assembler and follows a overlap-based string graph approach.
Thanks to its FM-index representation and Burrow-Wheeler compressing transformation, it manages considerable economy in memory.

The overlap graph is simplified before being transformed into the string graph used to resolve contig assembly.
K-mer correction for sequencing error removal, duplicated reads removal and ``bubble-popping'' are applied beforehand.
Reconstruction of the genome is done by finding a walk through the graph, where each vertices (reads) are connected if the reads overlap with minimum proportion.
Finally, the paired-end reads are aligned to the contigs to guide their ordering and assess their copy-number through coverage analysis.

To benchmark the method, they firstly used C. elegans reads.
This genome easier than a human genome in several sense: haploid, small (100 Mbp).

An interesting metric used to explore the assembler performance was computed by sampling strings of varying sizes from the reference genome and counting the fraction that is found exactly in one of the contig.

SGA showed the highest value for long strings.

I thought one of the strength of overlap-based method would be its robustness to sequencing errors or polymorphism.
In my head two reads would overlap significantly even if a few bases are discordant.
However, here, the overlap definition is based on comparing suffixes/prefixes, meaning that exact match is necessary.

SGA was modified in @Malhotra2013 to report all paths instead of the consensus, and validate the breakpoints regions of their tumoral calls.

---

**AbySS**


## SV genotypers

@Eggertsson2019 improved their variant GraphTyper to integrate structural variants. 
They show that it can genotype SNVs, indels and SVs from short-read data in large-scale datasets.
The approach is very similar to BayesTyper: SVs are discovered with methods likes Manta, then genotyped.
Many of the input SVs might be errors but it's not a big problem because the genotyper can figure out if the variant is supported or not by mapped reads.
One limitation of this breakpoint-centric approach is that some informative reads might be discarded. 


## Others

**SVMerge** [@Wong2010] is a pipeline applying multiple detection methods and using local de novo assembly to refine breakpoints.
Written in Perl, SVMerge is supposedly flexible and modular to integrate new callers.

The pipeline is divided in  four modules: calling, filtering, merging and de novo assembly.
After calling, calls are filtered according to their intrinsic quality as well as their proximity to gaps (less 600bp), highly repetitive regions or centromere/telomere (less than 1 Mb).
Calls are then separated by type and merged.
After extension of their boundaries, SV are locally assembled (by default ABySS assembler) and the consensus aligned to the reference genome using Exonerate.
These alignments are finally parsed to retrieve convincing SVs.

They mention attempting to detect complex SV that they defined as simple deletion/duplication with additional inversions.

In the special situation of heterozygous deletion, read depth is also analyzed to be sure that these regions are not discarded due to the presence of one normal copy.
Still the clear limitation of this approach is its restriction to homozygous events.
At this stage the assembly is not suited for diploid genomes.

As another demonstration of their assembly validation, only 11% of their insertion could be validated.
To conclude on the subject of limitations, they also filtered out inter-chromosomal translocation candidates because too numerous.

---

**CREST** [@Wang2011] uses both discordant paired-mapping and split-reads to call SVs.

---

**DELLY** [@Rausch2012] DELLY integrates information from short insert paired-reads and long-range mate-pairs as well as split-reads alignments.
It can theoretically detect any type of structural variation.
Still I'm suspecting the detection of duplications to be challenging with this approach.
They stress the advantage of mixing two sets of reads with different insert sizes.

The PEM side of the algorithm follows previous approaches: from the distribution of insert size, uniquely mapped read pairs are flagged as discordant if farther away than 3 times the distribution SD, or with discordant orientations.
Depending on the orientations and insert size the type of event is estimated: deletion, inversion, tandem duplication or translocation.
After being grouped by chromosome and sorted, the discordant mapping are used to build a graph with each node being a read and an edge meaning that they support the same SV.
Identification of SV is then done by looking up for cliques or sub-cliques.
Each type of event is analyzed separately.
These putative SVs are then screened for supporting split-reads.
This step first identify one-anchored reads (i.e. with one unmapped read).
They transformed the reference sequence in non-deletion to fit a deletion situation and developed a k-mer approach to avoid expensive exploration of all possible splits.
The exact breakpoint of a SV is found by comparing diagonal segment when mapping the reads k-mers to the reference sequence.
Obviously this approach is penalized by insertion of new sequences at the breakpoint location, especially if longer than half the read length.
Two split-reads are usually required to call a SV.
Finally the supporting split-reads are used to create a consensus sequence which re-aligned to the reference genome and define the final SV boundaries.
Additionally DELLY checks that both PEM and SR derived event are concordant.

They noticed an accumulation of putative calls in centro- and telomeric regions.
Believing it could be actual inter-individual variability or due to incomplete reference they didn't completely filter these calls out.
Still, they limited the number of variant in any particular genomic region in order to avoid countless alignments.

---

**SVM$^2$** [@Chiara2012] uses Support Vector Machine integrating coverage and paired-end read information.

$+++$

---

**forestSV** [@Michaelson2012] trained a random forest machine learning model with known variant. Different signals such as coverage, read-pair signature and others are integrated in the model. After training the method scans a BAM file to find probable variants.

---

**GASVPro** [@Sindi2012] integrates read depth and paired-end signal in their model and account for multiple alignments.

Apparently it weights multiple mapping per read in proportion to their probability [@Halper-Stromberg2014].

$+++$

---

**LUMPY** [@Layer2014] integrates multiple SV signals such as coverage, PEM and split-reads. It can be extended to new custom SV signals and can analyze several samples at the same time.

---

**ERDS** [@Zhu2012] integrates both coverage and B allele frequency in a Hidden Markov Model.

---

**@Yang2013** integrates both paired-end information and split-reads detect somatic SV in cancer genomes.

---

**SMUFFIN** [@Moncunill2014] analyzes directly the reads to find differences between pattern in the normal and tumor samples. Once somatic patterns are identified they can be mapped back to the reference genome.

---

**SMaSH** [@Talwalkar2014] is a toolkit to benchmark human genome variant calling. I think it's also giving calls, maybe using a consensus of its preferred methods ?

---

**SlideSort-BPR** [@Wijaya2014] is based on a fast all-against-all read comparison to find patterns of breakpoints.

---

**CLImAT** [@Yu2014] looks at both RD and B allele frequency. Quantile normalization is used on the B allele frequency.

---

**Parliament** [@English2015] is a consensus SV-calling infrastructure that merges multiple data types and SV detection methods. For example it can merge results from short and long read sequencing, as well as aCGH.


## Benchmark

IS:Insert-size; Or:Orientation

| Year | Method                                       | Approach     | Note                                    | Mult. samples | Event size           |
|  --- | ---                                          | ---          | ---                                     | ---           | ---                  |
| 2007 | @Korbel2007                        | PEM          | IS/Or                                   | no            | >~3kb                |
| 2008 | @Campbell2008                        | PEM, RD      | IS/Or, CBS                              | no            | CNV>~30kb            |
| 2008 | SeqSeg [@Chiang2009]                  | RD           | 2 bins around breakpoints               | case-control  | >~29kb               |
| 2009 | EWT [@Yoon2009]                       | RD           | Event-Wise Testing                      | no            | >~1kb                |
| 2009 | CNV-seq [@Xie2009]                    | RD           | theoretical                             | case-control  | NA                   |
| 2009 | BreakDancer [@Chen2009]               | PEM          | nbReads+IS+coverage, Poisson sig.       | >~10bp        |                      |
| 2009 | Pindel [@Ye2009]                      | SR           | Pattern-Growth                          | no            | D:1bp-10kb, I:1-20bp |
| 2010 | CNAseg [@Ivakhno2010]                 | RD           | HMM, flowcell varibility, Skellam dist. | no            | NA                   |
| 2010 | VariationHunter-CR [@Hormozdiari2010] | PEM          | Transposon model                        |               |                      |
| 2010 | SVMerge [@Wong2010]                   | PL           | *de novo* local assembly                |               |                      |
| 2010 | NovelSeq [@Hajirasouliha2010]         | PEM+Assembly |                                         |               |                      |
| 2011 | JointSLM [@Magi2011]                  | RD           | HMM, Common SVs                         | Yes           | >~500bp              |
| 2011 | Control-FREEC [@Boeva2011]            | RD+BAF        | CNV and contamination                   | single, pair  |  NA                    |
| 2011 | BIC-seq [@Xi2011]                     | RD           |                                         |               |                      |
| 2011 | Genome STRIP [@Handsaker2011]         | PEM          | Population-based                        |               |                      |
| 2011 | CNVnator [@Abyzov2011]                | RD           |                                         |               |                      |
| 2012 | VarScan 2 [@Koboldt2012]              | RD           | Control, exome                          |               |                      |
| 2012 | cn.MOPS [@Klambauer2012]              | RD           | MS, Bayesian                            |               |                      |
| 2012 | CLEVER [@Marschall2012]               | PEM          | Graphs                                  |               |                      |
| 2012 | Splitread [@Karakoc2012]              | SR           | Exome                                   |               |                      |
| 2012 | YAHA [@Faust2012]                     | SR           | Long reads                              |               |                      |
| 2012 | SVM2 [@Chiara2012]                    | RD+PEM       | SVM                                     |               |                      |
| 2012 | GASVPro [@Sindi2012]                  | RD+PEM       |                                         |               |                      |
| 2012 | LUMPY [@Layer2014]                    | PEM+SR       |                                         |               |                      |
| 2012 | DELLY [@Rausch2012]                   | PEM+SR       |                                         |               |                      |
| 2013 | WaveCNV [@Holt2013]                   | RD           | Cancer                                  |               |                      |
| 2013 | CNV-TV [@Duan2013a]                   | RD           | Segmentation                            | single, pair  | > 1Kbp               |
| 2013 | PeSV-Fisher [@Escaramis2013]          | PEM+RD       |                                         |               |                      |
| 2013 | CNVeM [@Wang2013]                     | RD           |                                         |               |                      |
| 2013 | CNVfinder [@McCallum2013]             | RD           | HMM, GC/mappability                     | no            | NA                   |
| 2013 | GENSENG [@Szatkiewicz2013]            | RD           | HMM, GC/mappability                     | no            | >800 bp              |
| 2014 | CLImAT [@Yu2014]                      | RD+BAF       |                                         |               |                      |
| 2014 | Gustaf [@Trappe2014]                  | SR           |                                         |               |                      |
| 2014 | SMASH [@Talwalkar2014]                | PEM+SR       |                                         |               |                      |
| 2015 | GROM-RD [@Smith2015]                  | RD           | Aggressive normalization                | no            | 500 bp++             |

| Year | Method                                       | GC correction | Map correction | CentTelomere dist. |
|  --- | ---                                          | ---           | ---            | ---                |
| 2007 | @Korbel2007                        | NA            | NA             | 0                  |
| 2008 | @Campbell2008                        | none          | binning        | 1Mb                |
| 2008 | SeqSeg [@Chiang2009]                  | unnecessary   | unnecessary    | NA                 |
| 2009 | EWT [@Yoon2009]                       | median        | random mapping | NA                 |
| 2009 | CNV-seq [@Xie2009]                    | unnecessary   | unnecessary    | NA                 |
| 2009 | BreakDancer [@Chen2009]               | NA            | NA             | pooled possible    |
| 2009 | Pindel [@Ye2009]                      | NA            | NA             | NA                 |
| 2010 | CNAseg [@Ivakhno2010]                 | LOESS         | DWT smoothing  | NA                 |
| 2010 | VariationHunter-CR [@Hormozdiari2010] |               |                |                    |
| 2010 | SVMerge [@Wong2010]                   |               |                |                    |
| 2010 | NovelSeq [@Hajirasouliha2010]         |               |                |                    |
| 2011 | JointSLM [@Magi2011]                  | median        | NA             | 0                  |
| 2011 | Control-FREEC [@Boeva2011]            | poly model    | track + exc    | 50 Kbp             |
| 2011 | BIC-seq [@Xi2011]                     |               |                | *In theory*, so No |
| 2011 | Genome STRIP [@Handsaker2011]         |               |                | Yes                |
| 2011 | CNVnator [@Abyzov2011]                |               |                |                    |
| 2012 | VarScan 2 [@Koboldt2012]              |               |                |                    |
| 2012 | cn.MOPS [@Klambauer2012]              |               |                |                    |
| 2012 | CLEVER [@Marschall2012]               |               |                |                    |
| 2012 | Splitread [@Karakoc2012]              |               |                |                    |
| 2012 | YAHA [@Faust2012]                     |               |                |                    |
| 2012 | SVM2 [@Chiara2012]                    |               |                |                    |
| 2012 | GASVPro [@Sindi2012]                  |               |                |                    |
| 2012 | LUMPY [@Layer2014]                    |               |                |                    |
| 2012 | DELLY [@Rausch2012]                   |               |                |                    |
| 2013 | WaveCNV [@Holt2013]                   |               |                |                    |
| 2013 | CNV-TV [@Duan2013a]                   | median        | random mapping | NA                 |
| 2013 | PeSV-Fisher [@Escaramis2013]          |               |                |                    |
| 2013 | CNVeM [@Wang2013]                     |               |                |                    |
| 2013 | CNVfinder [@McCallum2013]             | model         | model          | NA                 |
| 2013 | GENSENG [@Szatkiewicz2013]            | model         | model          | NA                 |
| 2014 | CLImAT [@Yu2014]                      |               |                |                    |
| 2014 | Gustaf [@Trappe2014]                  |               |                |                    |
| 2014 | SMASH [@Talwalkar2014]                |               |                |                    |
| 2015 | GROM-RD [@Smith2015]                  | quantile      |  quantile      | Yes or no          |

## Misc

### Mapping algorithm effect

Except for assembly approaches, the choice of read mapping algorithm might affect the results.
Methods that manipulate discordant mapping in particular might gain the most at using a mapping strategy that fits best their approach.

For example in @Malhotra2013, reads were realigned with *Novoalign* before investigating the discordant reads with *HYDRA-MULTI*.


### Comparing SV calls

GASV was developed to model uncertainty in the breakpoints [@Sindi2009].
Each supporting read/probe/call is represented as a polygon defined by the start/end location and error (e.g. from insert size distribution).
Polygons are then intersected using an efficient plane sweep to identify subset of intersecting polygons, that are assumed to support the same variant.
Most of the paper describes GASV on raw data but they mention that it can be used to compare SV calls across studies.

VarSim first normalize VCFs to canonical forms before comparison [@Mu2015].
Each record is converted to a simple insertion or deletion followed by SNVs. 
The insertion/deletion is either at the start/end of the variant, whatever results in the least number of mismatches.

[Wittler et al. (2013)](https://www.ncbi.nlm.nih.gov/pubmed/23369161) describe a clustering approach to better define overlapping deletions from paired-end reads.

[Leung et al. (2015)](https://www.ncbi.nlm.nih.gov/pubmed/25887570) propose a benchmarking tool, SV-AUTOPILOT, that tests SV calling methods. 
I should read how they overlap the SVs called and their truth sets.

[Sedlazeck et al. (2017)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5668921/) describe SURVIVOR a tool to annotate and compare different SV callsets.
It builds on [SURVIVOR](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5286201/) to fine-tune breakpoints.

The Simpson lab has a [blog post](http://simpsonlab.github.io/2015/06/15/merging-sv-calls/) on the topic of merging SV calls from different callers.
They used a window-based approach were calls at less than 300 bp from each other were merged.

A few methods use normalization approaches to transform variants in VCF into a simpler or more comparable variant.
Variant normalization is described in [this wiki](https://genome.sph.umich.edu/wiki/Variant_Normalization).
[vcflib](https://github.com/vcflib/vcflib#vcfallelicprimitives) has a `vcfallelicprimitives` function.

[JASMINE](https://github.com/mkirsche/Jasmine) (Jointly Accurate Sv Merging with Intersample Network Edges) can merge SVs from different samples (or methods?) using a network approach.
In the network representation, SVs are merged using a minimum spanning forest algorithm (*not sure yet how, must watch the RECOMB talk*).
It offers multiple ways to specify distance thresholds such as absolute, relative to variant size, per variant in the VCF INFO field.
Insertions can also be considered based on their sequence identity.

### In Silico validation

**Reads realignment** *targetSeqView* [@Halper-Stromberg2014] was developed to visualize and assess the quality of structural variants, especially in regions with repeats.
The goal is to distinguish true SV from alignment artifacts.

The reads are re-aligned to the SV sequence and the two contiguous (reference) sequences.
The score represents how better the reads map to the SV sequence compared to the reference sequences.
It relies on a binomial distribution that takes into account the base position in the read.

Reads mapping to each sides of the SV junction are realigned to the 3 sequences.
The final score is the log-likelihood of the probability in the rearranged reference versus the ones in the contiguous sequences.

They focused their validated on the Ig and TCR loci in lymphoid cancer.
These regions have a segmentally duplicated architecture that are difficult to assay using NGS.

It's a R package, and the re-alignment is performed using Smith-Waterman algorithm from the Biostrings package.
For their approach they recommend first using a NGS aligner that report multiple alignments per reads.
Other strategies might be better for RD analysis but reduce the pool of potential supporting reads mapping locally to the region of interest.

Some visualization tools described below can also assist in validating SVs.

### Visualization

[covviz](https://github.com/brwnj/covviz) visualizes coverage across multiple samples highlighting region where it diverges significantly from the other samples.
It's an interactive app that seems to use Plotly.
It slick!
Additional QC features are offered like sex prediction, PCA and low/variable coverage metrics per sample.
Brent Pedersen contributed to it which might explain why it can make use of *indexcov* (fast coverage computation from indexes; part of [goleft](https://github.com/brentp/goleft).
Apparently distributed/implemented by [Base2Genomics](https://base2genomics.com/), a company which emphasizes looking at all variation in a genome including SVs and STRs.

[SVPV](https://github.com/VCCRI/SVPV) (Structural Variant Prediction Viewer) visualizes alignments of paired-end reads around input SVs.
Useful to inspect calls and compare multiple callsets with automated PDF creation.
It creates a multi-panel figure showing depth, insert-size clustered pairs, soft clip presence.

# Bibliography

