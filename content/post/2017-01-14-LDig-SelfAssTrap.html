---
date: 2017-01-14
title: The Self-Assessment Trap
tags: ['literature']
output: blogdown::html_page
---



<p>I just read a short correspondence about the self-assessment trap, i.e.Â when <strong>the author is the judge, jury and executioner</strong> of comparing his method against existing methods.
One visible outcome is that most methods are found to be the <em>best</em>, which is unlikely to be true.</p>
<p>A typical explanation for this bias is the selection by the authors of just the metrics in which their method is better.
Different aspects of the same problem are emphasized in different papers and used for benchmarking.</p>
<p>Sometimes it is difficult to benchmark methods, especially in biological sciences.
The data is too small, or the expectation is not completely clear, or the experimental validation is partial or too unreliable.</p>
<p>They propose a few guidelines to avoid this trap:</p>
<ul>
<li>Use third-party validation and previously unseen data.</li>
<li>Use more than one metric to evaluate the methods.</li>
<li>Report good methods even if not the best.</li>
<li>Stress the impartial assessment to editors/reviewers.</li>
<li>Change scientific culture to value follow-up studies to confirm/refute results.</li>
</ul>
<p>They then mention some initiatives (<a href="http://www.the-dream-project.org">DREAM</a>, <a href="http://predictioncenter.org/">CASP</a> or <a href="http://www.ebi.ac.uk/msd-srv/capri/">CAPRI</a>) that address at least the first 3-4 guidelines.</p>
<p>Source: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3261704">The self-assessment trap: can we all be better than average? by Norel et al.</a></p>
