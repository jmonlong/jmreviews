---
date: 2019-06-17
title: "1PE - Week 1(-2)"
tags: ['literature']
output: blogdown::html_page
---

I'm going to try reading one paper everyday (1PE). 
This is week 1 (and 2 because I failed pretty quickly...).

## A robust benchmark for germline structural variant detection

[bioRxiv 2019](https://www.biorxiv.org/content/10.1101/664623v2)

This is the preprint describing the effort by the Genome in a Bottle consortium to produce a **SV gold standard** on HG002.
They used a lot of SV callers and different technologies on HG002 and his parents.

- Their tier 1 set contains ~10K SVs.
- They used a step-wise filtering pipeline to keep variants that
    - have no duplicates.
	- are supported by 2 technologies or 5 callers or BioNano support in trio.
	- were genotyped with enough support in HG002 by svviz.
	- were not complex (i.e. isolated).
	- were located in region with confident diploid assembly.
- They found more large insertions in tandem repeats than large deletions. Is it because TRs are collapsed in the reference genome? Or just the general deletion bias of the reference?
- The support for the alternate allele was around 50% for long reads but only ~30% for short reads, another good argument about the long reads lack of reference bias.
- Comparison with additional long read datasets showed that some 5% of insertions might still be missing from their truth set.

They used [SVAnalyzer](https://github.com/nhansen/SVanalyzer) **to cluster SVs from different caller while taking repeats into account**.
This new method compares haplotypes sequences of the variants and cluster SVs by proportion of divergent sequences.
That sounds very interesting and it has a *benchmark* module to evaluate a call set vs a truth set (although they used something different for evaluation in this paper).
Clusters are defined as connected components in a network constructed with this similarity measure.
However it's not clear how the SVs from the same cluster are merged.
From reviewing the Perl code it seems like the representative SV for a cluster is randomly selected from the largest subcluster of exactly matched variants (eventually randomly selected if multiple subcluster of maximal size).

Variants were also evaluated and genotyped with [svviz2](https://github.com/nspies/svviz2) to keep variants with enough long reads supporting the alternate allele.
**svviz2** is a new version of svviz that can not only visualize support for SVs but also evaluate and genotype SVs in an unsupervised manner.

They used [truvari](https://github.com/spiralgenetics/truvari) **to compare a test set with this truth set**.
Very similar to what we are doing with sveval but as before I find the criteria too lax.
The called variant needed to be within 30% of the true size and located at less than 2 kbp.

Future directions include producing similar dataset for GRCh38 and using better long-read sequencing (CCS PacBio and ultra-long nanopore).

## rMETL: sensitive mobile element insertion detection with long read realignment

[Bioinformatics 2019](https://academic.oup.com/bioinformatics/advance-article-abstract/doi/10.1093/bioinformatics/btz106/5317164)

A very short application note promoting a MEI caller from long-reads. 
The language is unfortunately not always clear.
For that reason I didn't completely grasp the methodology although several cartoons were provided in supplementary materials.

Briefly, discordant reads are first extracted, more specifically the sequence that is discordant with the reference (and a few bps-long flanks), then clustered and aligned to the consensus sequence of known TE (L1, Alus, SVA). 
From this realignment, ME insertions and deletions are genotyped following standard heuristics.
Their comparison with Sniffles shows that rMETL is more sensitive, especially at low coverage. 
They used simulated and real data, using the 1KGP MEI catalog as gold standard.

**The breakpoint fine-tuning step is a bit vague.**
In the supplements, they mention something about deriving the breakpoint from the alignment as the average of the breaks?
I think that it's the trickiest part for MEI callers but also the most exciting feature that could be gained from long reads.

On a side note, the acronym is pretty bad: a few cherry picked letters that make up a word that sounds like another existing MEI caller (MELT).
**R**ealignment-based **M**obile **E**lement insertion detection **T**ool for **L**ong read, rMETL...

## Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations

[European Journal of Epidemiology 2016](https://doi.org/10.1007/s10654-016-0149-3)

An important point discussed is how we should study a question completely instead of focusing only on binary hypotheses (e.g. *is the difference significant?*).
Both in teaching and application, the aim is wrongly about testing the null hypothesis.
But a study should **not only report p-values but also effect size, confidence interval and exploration of the context in which the conclusions are valid**.

> A more refined goal of statistical analysis is to provide an evaluation of certainty or uncertainty regarding the size of an effect.

There are a number of ways that P-values might not be accurate. 
A p-value test how well the data fits **all** the assumptions of the null model, both statistical model and data generation. 
It includes the parameter(s) of interest but also other assumptions about the model or test. 
Even non-parametric methods depend on assumptions such as random sampling.
Methods such as sensitivity and bias analysis can investigate such problems.
It is important to keep in mind *hidden* assumptions and challenge them as well.

> A statistical model is much more than an equation with Greek letters.

There is also the assumption that the analysis was not performed only to test significance (**analysis bias**) or that the results were not reported only based on their significance (**reporting and publication bias**).
I understand the first one as trying out several tests/models until reaching significance.
The second might be testing several hypotheses and reporting only the ones that are significant.

They list misinterpretations that cover what the P-value means, what to conclude from them, how to combine them together (e.g. replication). 
Same for confidence intervals and power.

I was a bit annoyed by one type of misinterpretation that is more about the expression *"the chance that"*.
They argue that the *chance of making an error* is either 0% if you conclude right or 100% if not.
I understand it like the *probability of making an error from seeing our data* but apparently you are either right or wrong and that's it. 
A bit annoying to repeat that several times over several misconceptions.
It felt like nitpicking. Maybe it's important.

At the end they quote Fisher who said:

> No scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.

They mentioned [Bayesian posterior/credible intervals](https://en.wikipedia.org/wiki/Credible_interval), the Bayesian equivalent of confidence intervals (although different like P-values vs predictive posterior P-values).


## Representing and decomposing genomic structural variants as balanced integer flows on sequence graphs

[BMC Bioinformatics 2016](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1258-4)

Given a set of SV breakpoints, the approach described in this article can recreate the evolutionary history.
Briefly a **sequence graph is first build** using breakpoint and copy number information, for example **from short-read sequencing**.
The goal is then to **find the history graph that best fit this sequence graph**.
A history graph is multi-layered, each layer representing a change in the original genome (top) due to a rearrangement.
Nodes contain sequences; edges represent either adjacency in the genome or the evolution of the sequence through layers.

I think I understand the goal of the method but I skipped all the math. 
I was already failing my *1 Paper Everyday* at this point and it seem too big of a challenge.

## The presence and impact of reference bias on population genomic studies of prehistoric human populations

[bioRxiv 2018](https://www.biorxiv.org/content/10.1101/487983v2)

**Ancient DNA studies are more sensitive to reference bias** for several reasons:

- shorter reads because material degraded.
- low coverage because of the small amount of material.
- some methods to deal with the data like the "pseudo-haploid" approach.

The reference bias in variant calling can then impact frequency estimates and other population genetics analysis.

I'm not really sure why one would like to use the **"pseudo-haploid" approach**: picking one read randomly at each site to represent a haploid version of the SNP.
Maybe it makes downstream analysis easier.

Choosing the mapping stringency is a bit of a quandary: too stringent and the reference bias is exacerbated; too lax and multi-mapping impacts variant calling later.

One of the way the reference bias was tested was to modify reads with a reference nucleotide with the alternative allele. 
Then both the real reads and modified reads were pooled together, making sure that 50% supported each allele. 
They observed bias in all samples and apparently across the entire genome.

As expected:

- The larger the read the smaller the reference bias.
- The random sampling approach ("pseudo-haploid") is more sensitive to reference bias than diploid calling.

Interestingly, **the reference genome was also split into population of origin** (from a paper by Ed Green).
I had never seen this kind of partitioning before.
I should keep this in mind.
They found that segments of African origin were affected the least by reference bias and the opposite for European ancestry. 

The authors propose to filter reads to counter some potential reference bias. 
Reads would be kept only if changing the nucleotide of interest didn't change its mapping location.
The nucleotide of interest can be changed either in the read or the reference genome (using a third nucleotide than the ref/alt alleles).
This post-mapping correction helps but couldn't correct the bias completely.
That would require an unbiased mapping step (genome graphs!).

TIL of post-mortem deamination: deaminated nucleotides are then misread by DNA polymerase, looking like C->T or G->A mutations.

## CRISPR/Cas9-Mediated Scanning for Regulatory Elements Required for HPRT1 Expression via Thousands of Large, Programmed Genomic Deletions

[AJHG 2017](http://dx.doi.org/10.1016/j.ajhg.2017.06.010)

HPRT1 is a **housekeeping gene** (so expressed in every cell) but its regulation is not well understood.
Disruptive variants leads to the **X-linked Lesch-Nyhan syndrome**.
A small proportion of affected individuals have apparently no coding variants. 
Could it be because of non-coding variants?
**This study investigates which region around this gene causes down-regulation when deleted.**

Methods:

1. Pick pairs of available cut sites around HPRT1 creating overlapping tiles (4,342 pairs, 27x median coverage).
1. Introduce these cuts to create deletions.
1. Sequence before selection.
1. Grow the cells selecting for cells that don't express the gene.
1. Sequence again.
1. Compute enrichment scores for each pair.
1. Summarize that at the nucleotide level.
1. Identify region with high enrichment scores.
1. Profit.

The **"negative" results** showed that only deletions of coding regions and the very near promoter are selected for by the experiment.
That means that other deletions don't have a effect strong enough to be selected for or at least much lower than a coding deletion.
Distal non-coding variants causing subtle expression changes might still contribute to the disease but no low-hanging fruit.
At least not in this cell line. 
